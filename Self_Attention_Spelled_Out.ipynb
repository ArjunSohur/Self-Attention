{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPasEwPAYUpo0baUtIlh2kZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArjunSohur/Self-Attention/blob/master/Self_Attention_Spelled_Out.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self Attention Spelled Out"
      ],
      "metadata": {
        "id": "z8jM-bQZahha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we will review the structure of a multi head attention module using self attention.\n",
        "\n",
        "The goal in this notebook is not to create a training mechanism that trains the multihead attention weights (that will be abother notebook).  \n",
        "\n",
        "Moreover, the goal is to be able to see how self attention - and multi head attention by extention -  look and interact in code.\n",
        "\n",
        "This version of sef attention is not the most rubust, clean, nor efficient implementation; rather, it aims to be a gentle introduction to enable understanding more complicated architecture.\n",
        "\n",
        "We recommend that you understand self attention theoretically before trying to understand this code.  If you're unfamiliar with how attention works (particually the significance of queries, keys, and values), check out our guide to self attention:\n",
        "\n",
        "https://github.com/ArjunSohur/transformergallery/blob/main/README.md\n",
        "\n",
        "The code is heavily based off of the article:\n",
        "\n",
        "https://medium.com/the-dl/transformers-from-scratch-in-pytorch-8777e346ca51\n",
        "\n",
        "This article seemed to have the most consie and down-to-first-principles based self attention implementation we've seen.  Since it helped us in our journey to understanding multi head attention, we think it can greatly benefit you as well!\n",
        "\n",
        "Enjoy!"
      ],
      "metadata": {
        "id": "JBFIabIjaoyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "XR6WY4z4cNNJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6qH4_P2TaHq0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as f"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self Attention Head"
      ],
      "metadata": {
        "id": "r9xyFVL-ciLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with implementing a single self attention head.  The most common formula for self attention is the scaled dot product attention: $$\\text{attention(Q, K, V)} = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_{QK}}})V$$\n",
        "Where $Q, K \\in \\mathbb{R}^{n \\text{ } \\times \\text{ } d_{QK}} \\text{ and } V \\in \\mathbb{R}^{n \\text{ } \\times \\text{ } d_V } $.  If you are confused about what these $Q, K, \\text{ and } V$ come from, check out our explanation:\n",
        "https://github.com/ArjunSohur/transformergallery/blob/main/README.md\n",
        "\n",
        "Our first task will be creating this formula.  The biggest challenge - and one that we will encounter quite often - will be making sure that the dimensions of our matrices match up for matric multiplication."
      ],
      "metadata": {
        "id": "Y4QQEZYfclbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PARAMS\n",
        "# queries - tensor of shape (batch_size, sequence_length, number_of features_for_queries)\n",
        "# keys - tensor of shape (batch_size, sequence_length, number_of features_for_keys)\n",
        "# Note: number_of features_for_keys = number_of features_for_queries\n",
        "# values - tensor of shape (batch_size, sequence_length, number_of features_for_values)\n",
        "def scaled_dot_product_attention(queries: Tensor, keys: Tensor, values: Tensor) -> Tensor:\n",
        "    # First, we batch matrix multiply the queries vector with the transpose of the keys vector\n",
        "    # This step essentially determines how important each position is relative to each other\n",
        "    # Results in a sequence_length x sequence_length matrix\n",
        "    matrix_mult_of_queries_and_keys = queries.bmm(keys.transpose(1, 2))\n",
        "\n",
        "    # We need to ensure that back propagation runs smoothly\n",
        "    # We keep the values of our above matrix multiplication in check by dividing by the square root\n",
        "    # of the number of hidden dimension of the queries (which equals the hidden dimension of the keys)\n",
        "    scalar_for_gradient_stability = queries.size(-1) ** (1/2)\n",
        "\n",
        "    # Dividing the values of the matrix multiplication by the number of hidden dimension of the queries\n",
        "    matrix_multiplication_adjusted_by_scalar = matrix_mult_of_queries_and_keys / scalar_for_gradient_stability\n",
        "\n",
        "    # To further help gradient descent and to standardize weights, we apply softmax row-wise on out result\n",
        "    softmax_scaled_matrix_multiplication = f.softmax(matrix_multiplication_adjusted_by_scalar, dim=-1)\n",
        "\n",
        "    # Lastly, we perform batch matrix multiplication with the value matrices\n",
        "    scaled_dot_product_attention_result = softmax_scaled_matrix_multiplication.bmm(values)\n",
        "\n",
        "    # Results in a sequence_length x number_of features_for_values matrix\n",
        "    return scaled_dot_product_attention_result"
      ],
      "metadata": {
        "id": "fCMJlj1ZcRBG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the formula down, we can actually focus on the self attention head itself.  We represent it as a class so that we can instantiate as many separate self attention mechanisms as possible.\n",
        "\n",
        "There are two methods in our self attention class: the instantiation method and the the forward method.\n",
        "\n",
        "The initialization method defines linear, single-layer neural networks that will act as the weigths for our ** queries, keys, and values **.  We can view a neural network as weights if we think about the weigths of the edges of a fully connected neural network between each node as an element of a matrix.  Each row corresponds to an input nodes edge weight with the output nodes.  The neural network is important for traning the weights so that the attention mechanism leans what to pay attention to.\n",
        "\n",
        "The forward method passes the query, key, and value inputs through the weighting neural networks then sends the result to the sclaed dot product attention formula."
      ],
      "metadata": {
        "id": "3XzQu9ewg4W-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "    # PARAMS\n",
        "    # Weight matrices are crucial for the attention model to learn what is important and what isn't\n",
        "    # All the parameters make it so that matrix multiplication of with the queries, keys, and values goes smoothly\n",
        "    # Considering the queries, keys, and values matrices will be sequence_length x embedding_length\n",
        "    # we use the embedding length as the number of rows and chose a number for the amount of hidden dimensions\n",
        "    def __init__(self, embedding_dimension: int, queries_keys_hidden_dimension: int, values_hidden_dimension: int):\n",
        "        # Since SelfAttentionHead is a subclass of nn.module, we need to make a super call\n",
        "        super(SelfAttentionHead, self).__init__()\n",
        "\n",
        "        # Weights for thr keys, queries, and values\n",
        "        self.query_weights = nn.Linear(embedding_dimension, queries_keys_hidden_dimension)\n",
        "        self.key_weights = nn.Linear(embedding_dimension, queries_keys_hidden_dimension)\n",
        "        self.value_weights = nn.Linear(embedding_dimension, values_hidden_dimension)\n",
        "\n",
        "    # Overriding nn.Module's forward method\n",
        "    # PARAMS\n",
        "    # query, key, and values are all have size input_sequence_length x embedding size\n",
        "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
        "        # performing matrix multiplication with the weights\n",
        "        weighted_query = self.query_weights(query)\n",
        "        weighted_key = self.key_weights(key)\n",
        "        weighted_value = self.value_weights(value)\n",
        "\n",
        "        # using scaled dot product attention to find the attention weights\n",
        "        attention = scaled_dot_product_attention(weighted_query, weighted_key, weighted_value)\n",
        "\n",
        "        return attention"
      ],
      "metadata": {
        "id": "gwJLp952n3us"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}